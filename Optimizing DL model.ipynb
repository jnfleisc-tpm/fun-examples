{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d0c6e5-dae2-47aa-8f2b-128b88aa32f5",
   "metadata": {},
   "source": [
    "# Optimizing Model Example\n",
    "**(c) Feb 2025 Julie Fleischer**\n",
    "\n",
    "This file contains optimizations to my model in Example Solution from Deep Learning Class.  Details on the optimizations I made (and the ah-ha moment insight I had that allowed me to get to 100% accuracy) are below.\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ec1bb-3b04-441c-9d71-91840eef1aea",
   "metadata": {},
   "source": [
    "## 1 - Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea29ee2-4426-4c57-b2bf-7d95be4f796c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install required libraries (if not already installed)\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c92f1f-cf6d-468c-9c83-5f569dc2977a",
   "metadata": {},
   "source": [
    "## 2 - Load and pre-process input data\n",
    "\n",
    "In this step, we load the data in from input file.\n",
    "\n",
    "See next step for details on why the input file changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb3c9f10-4b8a-4f86-8b0a-b9febdc788f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------------- Input file has been loaded -----------------\n",
      "\n",
      " ----------------- Converted root cause to numeric -----------------\n",
      "\n",
      " ----------------- Data in numpy array -----------------\n",
      "\n",
      " ----------------- X data and Y data extracted -----------------\n",
      "\n",
      " ----------------- Y data converted to binary matrix -----------------\n",
      "\n",
      " ----------------- Data split into training and test data -----------------\n",
      "\n",
      "Training data shapes: (900, 7) (900, 3)\n",
      "Test data shapes: (100, 7) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load and pre-process input data\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the file\n",
    "#root_cause_data = pd.read_csv('deep_learning_sample_data.csv')\n",
    "root_cause_data = pd.read_csv('deep_learning_sample_data_better.csv') # new file loaded - see next step for why\n",
    "\n",
    "print(\"\\n ----------------- Input file has been loaded -----------------\\n\")\n",
    "\n",
    "# Convert ROOT_CAUSE column (the target data column) from string to ordinal number\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "root_cause_data['ROOT_CAUSE'] = le.fit_transform(root_cause_data['ROOT_CAUSE'])\n",
    "print(\" ----------------- Converted root cause to numeric -----------------\\n\")\n",
    "\n",
    "# Create a numpy array from root_cause_data for use with Keras functions\n",
    "\n",
    "root_cause_data_np = root_cause_data.to_numpy()\n",
    "print(\" ----------------- Data in numpy array -----------------\\n\")\n",
    "\n",
    "# Create our input data (X data) array from columns 2-8 (the seven boolean columns)\n",
    "# and our target data (Y data) from the last column (the ROOT_CAUSE) column\n",
    "\n",
    "X_data = root_cause_data_np[:, 1:8]\n",
    "Y_data = root_cause_data_np[:, 8]\n",
    "print(\" ----------------- X data and Y data extracted -----------------\\n\")\n",
    "\n",
    "# Convert ROOT_CAUSE column (the target data column) from ordinal number to boolean matrix using one hot encoding\n",
    "\n",
    "import tensorflow as tf\n",
    "Y_data = tf.keras.utils.to_categorical(Y_data)\n",
    "print(\" ----------------- Y data converted to binary matrix -----------------\\n\")\n",
    "\n",
    "# Split data into training data and test data.  Use 10% of the data for test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.1)\n",
    "print(\" ----------------- Data split into training and test data -----------------\\n\")\n",
    "print(\"Training data shapes:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test data shapes:\", X_test.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182b831-d910-4372-857d-21f54011ba26",
   "metadata": {},
   "source": [
    "## 3 - Create the Deep Learning Model\n",
    "\n",
    "The optimization journey for this model is described below.\n",
    "\n",
    "The original model (from file Example Solution from Deep Learning Class) used the following design decisions:\n",
    "- I started with the best practice a single hidden layer with a number of nodes equal to the average of input and output layer sizes, a sigmoid activation function on hidden layers (to mirror the binary data), softmax activation function on the output layer, and a categorical cross-entropy loss function (since this was a multi-class classification).  Ultimately, I ended up adding one more layer and a few more input nodes.\n",
    "- I couldn't get an accuracy higher than ~85% training and testing (on the class example data set) and 86% during training and 76% during testing (on my data set).\n",
    "\n",
    "Per community feedback (thanks to chatGPT), I started playing around more with optimizations to this model.  Those are described below:\n",
    "- I added more neurons in the first layer and made those a multiple of 2 (like 16, 32).\n",
    "- I added another layer.\n",
    "- I played around with the activation function on hidden layers and moved from sigmoid to ReLU.\n",
    "\n",
    "My findings from these optimizations were:\n",
    "- A third hidden layer helped with accuracy.  A fourth made it worse.\n",
    "- The best combination for neurons was 32 (layer 1), 32 (layer 2), 16 (layer 3).  I could get 78% accuracy during testing in that case (although only 83% during training).\n",
    "- ReLU didn't help the accuracy, so I went back to sigmoid.  It may have helped if I had converted the input from binary to float, which I didn't end up trying out.\n",
    "\n",
    "---\n",
    "That said, none of these really moved the needle much.  This surprised me since I created the input file, and I knew I was using a basic linear function to directly map my independent variables to the target.  Given that I (for the sake of the exercise) gave my model data with no errors, I had expected it to eventually get to 100% accuracy.  Since it wasn't, I went back and explored my data.\n",
    "\n",
    "It turns out, when I thought I was using binary values for all independent variables to generate an integer from 0-2 (which I then converted to a string), I was actually using floating point values.  I just didn't notice this because I had formatted Excel to print zero decimal places.\n",
    "\n",
    "I went back and converted all my independent variables to true binary values (which were true zeros and ones) and then applied my linear function to them.  **Once I ran the model on my new data set, I got to 100% accuracy very quickly (~70 epochs).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fed0b7b1-c445-4286-99e8-b68629b3459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " HiddenLayer1 (Dense)        (None, 32)                256       \n",
      "                                                                 \n",
      " HiddenLayer2 (Dense)        (None, 32)                1056      \n",
      "                                                                 \n",
      " HiddenLayer3 (Dense)        (None, 16)                528       \n",
      "                                                                 \n",
      " OutputLayer (Dense)         (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1891 (7.39 KB)\n",
      "Trainable params: 1891 (7.39 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the DL model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "HIDDEN_LAYER1_NODES = 32\n",
    "HIDDEN_LAYER2_NODES = 32\n",
    "HIDDEN_LAYER3_NODES = 16\n",
    "#HIDDEN_LAYER4_NODES = 16 # removed because decreases accuracy\n",
    "\n",
    "# Create a simple sequential model that takes all 7 columns of input and delivers one of the three target output values\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(HIDDEN_LAYER1_NODES, input_shape=(7,), name='HiddenLayer1', activation='sigmoid'), \n",
    "    layers.Dense(HIDDEN_LAYER2_NODES, name='HiddenLayer2', activation='sigmoid'), \n",
    "    layers.Dense(HIDDEN_LAYER3_NODES, name='HiddenLayer3', activation='sigmoid'), \n",
    "#    layers.Dense(HIDDEN_LAYER3_NODES, name='HiddenLayer4', activation='sigmoid'), \n",
    "    layers.Dense(3, name='OutputLayer', activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with categorical cross-entropy where we monitor accuracy\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322c462-898b-42bd-aa13-a2305d1f369a",
   "metadata": {},
   "source": [
    "## 4 - Train and Evaluate Model\n",
    "\n",
    "We can now train and evaluate the model.  I used the following heuristics when choosing the hyperparameters for training:\n",
    "- Batch size to be a power of 2\n",
    "- Started with an epoch size of 10 and grew to 300 to increase accuracy.\n",
    "- Validation split of 0.2 per best-practice to have roughly 20% of data to be validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c5127f6-9758-4b31-b33a-48e5ebe5d667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------------- Starting training -----------------\n",
      "\n",
      "Epoch 1/70\n",
      "45/45 [==============================] - 1s 5ms/step - loss: 1.0179 - accuracy: 0.4264 - val_loss: 1.0410 - val_accuracy: 0.3889\n",
      "Epoch 2/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 1.0106 - accuracy: 0.4306 - val_loss: 1.0388 - val_accuracy: 0.3889\n",
      "Epoch 3/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 1.0080 - accuracy: 0.4458 - val_loss: 1.0363 - val_accuracy: 0.3889\n",
      "Epoch 4/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 1.0054 - accuracy: 0.4639 - val_loss: 1.0355 - val_accuracy: 0.3889\n",
      "Epoch 5/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 1.0017 - accuracy: 0.4514 - val_loss: 1.0291 - val_accuracy: 0.3889\n",
      "Epoch 6/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.9975 - accuracy: 0.4403 - val_loss: 1.0202 - val_accuracy: 0.6722\n",
      "Epoch 7/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.9904 - accuracy: 0.4833 - val_loss: 1.0145 - val_accuracy: 0.6111\n",
      "Epoch 8/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.9827 - accuracy: 0.5222 - val_loss: 1.0046 - val_accuracy: 0.4222\n",
      "Epoch 9/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.9689 - accuracy: 0.5083 - val_loss: 0.9876 - val_accuracy: 0.7167\n",
      "Epoch 10/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.9509 - accuracy: 0.6292 - val_loss: 0.9650 - val_accuracy: 0.6056\n",
      "Epoch 11/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.9268 - accuracy: 0.5611 - val_loss: 0.9399 - val_accuracy: 0.7444\n",
      "Epoch 12/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.8989 - accuracy: 0.7014 - val_loss: 0.9053 - val_accuracy: 0.7111\n",
      "Epoch 13/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.8550 - accuracy: 0.7458 - val_loss: 0.8573 - val_accuracy: 0.6389\n",
      "Epoch 14/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.8000 - accuracy: 0.6958 - val_loss: 0.7919 - val_accuracy: 0.7667\n",
      "Epoch 15/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.7439 - accuracy: 0.7667 - val_loss: 0.7401 - val_accuracy: 0.6833\n",
      "Epoch 16/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.7653 - val_loss: 0.6768 - val_accuracy: 0.7333\n",
      "Epoch 17/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6256 - accuracy: 0.7750 - val_loss: 0.6217 - val_accuracy: 0.7389\n",
      "Epoch 18/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5698 - accuracy: 0.7889 - val_loss: 0.5661 - val_accuracy: 0.8000\n",
      "Epoch 19/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5184 - accuracy: 0.8250 - val_loss: 0.5164 - val_accuracy: 0.8000\n",
      "Epoch 20/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4745 - accuracy: 0.8333 - val_loss: 0.4810 - val_accuracy: 0.7944\n",
      "Epoch 21/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.8389 - val_loss: 0.4422 - val_accuracy: 0.8389\n",
      "Epoch 22/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8722 - val_loss: 0.4137 - val_accuracy: 0.8222\n",
      "Epoch 23/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.9181 - val_loss: 0.3790 - val_accuracy: 0.8833\n",
      "Epoch 24/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3412 - accuracy: 0.9292 - val_loss: 0.3542 - val_accuracy: 0.8889\n",
      "Epoch 25/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3159 - accuracy: 0.9417 - val_loss: 0.3284 - val_accuracy: 0.9056\n",
      "Epoch 26/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2928 - accuracy: 0.9528 - val_loss: 0.3065 - val_accuracy: 0.9500\n",
      "Epoch 27/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2719 - accuracy: 0.9625 - val_loss: 0.2844 - val_accuracy: 0.9611\n",
      "Epoch 28/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.9722 - val_loss: 0.2675 - val_accuracy: 0.9778\n",
      "Epoch 29/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2377 - accuracy: 0.9764 - val_loss: 0.2510 - val_accuracy: 0.9778\n",
      "Epoch 30/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.9722 - val_loss: 0.2368 - val_accuracy: 0.9889\n",
      "Epoch 31/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9778 - val_loss: 0.2224 - val_accuracy: 0.9889\n",
      "Epoch 32/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1987 - accuracy: 0.9792 - val_loss: 0.2122 - val_accuracy: 0.9667\n",
      "Epoch 33/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.9833 - val_loss: 0.2007 - val_accuracy: 0.9722\n",
      "Epoch 34/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.9833 - val_loss: 0.1906 - val_accuracy: 0.9778\n",
      "Epoch 35/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 0.9861 - val_loss: 0.1807 - val_accuracy: 0.9889\n",
      "Epoch 36/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.9861 - val_loss: 0.1770 - val_accuracy: 0.9778\n",
      "Epoch 37/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9889 - val_loss: 0.1620 - val_accuracy: 1.0000\n",
      "Epoch 38/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1429 - accuracy: 0.9917 - val_loss: 0.1536 - val_accuracy: 1.0000\n",
      "Epoch 39/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.9917 - val_loss: 0.1575 - val_accuracy: 0.9611\n",
      "Epoch 40/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1304 - accuracy: 0.9903 - val_loss: 0.1411 - val_accuracy: 1.0000\n",
      "Epoch 41/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.9903 - val_loss: 0.1375 - val_accuracy: 1.0000\n",
      "Epoch 42/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1207 - accuracy: 0.9917 - val_loss: 0.1415 - val_accuracy: 0.9778\n",
      "Epoch 43/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.9931 - val_loss: 0.1293 - val_accuracy: 0.9889\n",
      "Epoch 44/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1121 - accuracy: 0.9958 - val_loss: 0.1283 - val_accuracy: 0.9778\n",
      "Epoch 45/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9944 - val_loss: 0.1448 - val_accuracy: 0.9333\n",
      "Epoch 46/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1035 - accuracy: 0.9944 - val_loss: 0.1176 - val_accuracy: 0.9778\n",
      "Epoch 47/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1006 - accuracy: 0.9972 - val_loss: 0.1095 - val_accuracy: 1.0000\n",
      "Epoch 48/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0975 - accuracy: 0.9944 - val_loss: 0.1181 - val_accuracy: 0.9722\n",
      "Epoch 49/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0929 - accuracy: 0.9917 - val_loss: 0.1024 - val_accuracy: 1.0000\n",
      "Epoch 50/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0902 - accuracy: 0.9931 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
      "Epoch 51/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0881 - accuracy: 0.9944 - val_loss: 0.1023 - val_accuracy: 1.0000\n",
      "Epoch 52/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9972 - val_loss: 0.0928 - val_accuracy: 1.0000\n",
      "Epoch 53/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9972 - val_loss: 0.0908 - val_accuracy: 1.0000\n",
      "Epoch 54/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9944 - val_loss: 0.0877 - val_accuracy: 1.0000\n",
      "Epoch 55/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9972 - val_loss: 0.0863 - val_accuracy: 1.0000\n",
      "Epoch 56/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0767 - accuracy: 0.9958 - val_loss: 0.0876 - val_accuracy: 1.0000\n",
      "Epoch 57/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9986 - val_loss: 0.1067 - val_accuracy: 0.9611\n",
      "Epoch 58/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9986 - val_loss: 0.0807 - val_accuracy: 1.0000\n",
      "Epoch 59/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9986 - val_loss: 0.0889 - val_accuracy: 0.9722\n",
      "Epoch 60/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9958 - val_loss: 0.0890 - val_accuracy: 1.0000\n",
      "Epoch 61/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9986 - val_loss: 0.0803 - val_accuracy: 0.9944\n",
      "Epoch 62/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9972 - val_loss: 0.0703 - val_accuracy: 1.0000\n",
      "Epoch 63/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9972 - val_loss: 0.0686 - val_accuracy: 1.0000\n",
      "Epoch 64/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9986 - val_loss: 0.0678 - val_accuracy: 1.0000\n",
      "Epoch 65/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9986 - val_loss: 0.0683 - val_accuracy: 1.0000\n",
      "Epoch 66/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9986 - val_loss: 0.0643 - val_accuracy: 1.0000\n",
      "Epoch 67/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9972 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
      "Epoch 68/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9986 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
      "Epoch 69/70\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9986 - val_loss: 0.0614 - val_accuracy: 1.0000\n",
      "Epoch 70/70\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9986 - val_loss: 0.0589 - val_accuracy: 1.0000\n",
      "\n",
      " ----------------- Training finished -----------------\n",
      "\n",
      "\n",
      " ----------------- Starting evaluation -----------------\n",
      "\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 1.0000\n",
      "\n",
      " ----------------- Evaluation results -----------------\n",
      "\n",
      "Delta between predicted and actual values for model (loss): 0.0393\n",
      "Accuracy for model: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "\n",
    "# Setting key hyperparameters as constants for easily modifying\n",
    "\n",
    "BATCH_SIZE = 16 # power of 2\n",
    "EPOCH_SIZE = 70 \n",
    "VALIDATION_SPLIT = 0.2 # roughly 20% of data is validation data\n",
    "\n",
    "# This is where training occurs.  For EPOCH_SIZE runs, we will run BATCH_SIZE data through the model to train.\n",
    "# After that, we'll run VALIDATION_SPLIT percentage of the training data through to fine tune the model.\n",
    "\n",
    "print(\"\\n ----------------- Starting training -----------------\\n\")\n",
    "model.fit(X_train, Y_train, epochs=EPOCH_SIZE, batch_size=BATCH_SIZE, verbose=1, validation_split=VALIDATION_SPLIT)\n",
    "print(\"\\n ----------------- Training finished -----------------\\n\")\n",
    "\n",
    "# This is where we test our model and see how it did.\n",
    "\n",
    "print(\"\\n ----------------- Starting evaluation -----------------\\n\")\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"\\n ----------------- Evaluation results -----------------\\n\")\n",
    "print(f\"Delta between predicted and actual values for model (loss): {loss:.4f}\")\n",
    "print(f\"Accuracy for model: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a1b17-a38f-4058-8fd7-db7ea72db699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087d2ad-221b-4ac4-9d8f-10a8938da16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
